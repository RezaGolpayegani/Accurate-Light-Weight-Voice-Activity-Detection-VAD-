{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qRhBRa8Kw1X"
      },
      "source": [
        "# This notebook trains a convolutional recurrent neural network (CRNN) on the synthesised data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQQNKxIE0EPh"
      },
      "outputs": [],
      "source": [
        "# Handle colab bug\n",
        "!pip install numba==0.48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iQeu-LxPaeV"
      },
      "outputs": [],
      "source": [
        "!pip install sed_eval\n",
        "!pip install librosa==0.7.2\n",
        "!pip install soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAr0CO_wC543"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "import math\n",
        "import glob\n",
        "import sed_eval\n",
        "import dcase_util\n",
        "import pickle\n",
        "import os\n",
        "import soundfile as sf\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2muBoiK7fMKP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Mount Google Drive into Colab.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpjtJhW3kRPA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extract the .zip files into the 'train data' folder.\n",
        "\"\"\"\n",
        "from zipfile import ZipFile\n",
        "\n",
        "for i in range(0, 8):\n",
        "  zip_name = \"/content/drive/My Drive/Data Synthesis/Train - d_\" + str(i + 1) + \".zip\"\n",
        "  with ZipFile(zip_name, 'r') as zip:\n",
        "    zip.extractall('train data')\n",
        "    print(\"Extracted all sound files into the folder {}\".format(i + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuftbJJSC-oZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extracting BBC+MuSpeak Val data\n",
        "\"\"\"\n",
        "from zipfile import ZipFile\n",
        "zip_name = \"/content/drive/My Drive/Data Synthesis/Val - d.zip\"\n",
        "with ZipFile(zip_name, 'r') as zip:\n",
        "  zip.extractall('validation data')\n",
        "  print(\"Extracted all sound files into the folder\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ8m-5Kl7JAb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_examples, batch_size=128, dim=(1, ),\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        print(\"Constructor called!!!\")\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_examples = list_examples\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        #print(\"The self.list_examples is {}\".format(self.list_examples))\n",
        "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "      self.indexes = np.arange(len(self.list_examples))\n",
        "      if self.shuffle == True:\n",
        "          np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # # Initialization\n",
        "\n",
        "        X = np.empty([self.batch_size, 802, 80], dtype=np.float32)\n",
        "        y = np.empty([self.batch_size, 802, 2], dtype=np.int16)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "\n",
        "            X[i,:, :] = np.load(ID[0])\n",
        "\n",
        "            # Store class          \n",
        "            y[i, :, :] = np.load(ID[1])\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kh1_gI_vKMO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions to perform natural sort on the examples and labels.\n",
        "\"\"\"\n",
        "import re\n",
        "\n",
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "    \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx3diPkTbs-W"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "\"\"\"\n",
        "Load the individual numpy arrays into partition\n",
        "\"\"\"\n",
        "data = glob.glob(\"/content/train data/content/Mel Files/**/mel-id-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(data)\n",
        "\n",
        "labels = glob.glob(\"/content/train data/content/Mel Files/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(labels)\n",
        "\n",
        "train_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(train_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phPv7YAbej_-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creating the train partition.\n",
        "\"\"\"\n",
        "m_train = 20480 * 2\n",
        "random.seed()\n",
        "random.shuffle(train_examples)\n",
        "\n",
        "data_MS = glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-[0-9]*.npy\", recursive=True) \n",
        "sort_nicely(data_MS)\n",
        "\n",
        "labels_MS = glob.glob(\"/content/train data/MuSpeak/content/Mel Files/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(labels_MS)\n",
        "\n",
        "train_examples_MS = [(data_MS[i], labels_MS[i]) for i in range(len(data_MS))]\n",
        "\n",
        "partition = {}\n",
        "partition['train'] = train_examples[0:m_train] + train_examples_MS\n",
        "\n",
        "random.shuffle(partition['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXPkenT6d9iL"
      },
      "outputs": [],
      "source": [
        "print(\"The size of partition['train'] is {}\".format(len(partition['train'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sianBGv5hkr0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This loads data for the validation set.\n",
        "\"\"\"\n",
        "import glob\n",
        "import random\n",
        "\n",
        "data = glob.glob(\"/content/validation data/**/mel-id-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(data)\n",
        "\n",
        "labels = glob.glob(\"/content/validation data/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "sort_nicely(labels)\n",
        "\n",
        "validation_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(validation_examples)\n",
        "print(validation_examples[0])\n",
        "\n",
        "partition['validation'] = validation_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWAsqzHqmncM"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# This loads data for the test set.\n",
        "# \"\"\"\n",
        "# import glob\n",
        "# import random\n",
        "\n",
        "# data = glob.glob(\"/content/test data/**/mel-id-[0-9]*.npy\", recursive=True)\n",
        "# sort_nicely(data)\n",
        "\n",
        "# labels = glob.glob(\"/content/test data/**/mel-id-label-[0-9]*.npy\", recursive=True)\n",
        "# sort_nicely(labels)\n",
        "\n",
        "# test_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
        "\n",
        "# random.seed(4)\n",
        "# random.shuffle(test_examples)\n",
        "# print(test_examples[0])\n",
        "\n",
        "# partition['test'] = test_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZzW248zAFTC"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "params = {'dim': (1, ),\n",
        "          'batch_size': 128,\n",
        "          'n_classes': 2,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], **params)\n",
        "validation_generator = DataGenerator(partition['validation'], **params)\n",
        "# test_generator = DataGenerator(partition['test'], **params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXJg3Gkm8JFq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, model_path, patience=0):\n",
        "    super(MyCustomCallback, self).__init__()\n",
        "    self.patience = patience\n",
        "    # best_weights to store the weights at which the minimum loss occurs.\n",
        "    self.best_weights = None\n",
        "    self.model_path = model_path\n",
        " \n",
        "  def on_train_begin(self, logs=None):\n",
        "    # The number of epoch it has waited when loss is no longer minimum.\n",
        "    self.wait = 0\n",
        "    # The epoch the training stops at.\n",
        "    self.stopped_epoch = 0\n",
        "    # Initialize the best F1 as 0.0.\n",
        "    self.best_val_loss = np.inf\n",
        "    self.is_impatient = False\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    if not self.is_impatient:\n",
        "      print(\"Restoring model weights from the end of the best epoch.\")\n",
        "      self.model.set_weights(self.best_weights)\n",
        "      temp_model_path = self.model_path.replace(\".h5\", \"_temp.h5\")\n",
        "      os.remove(temp_model_path)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    current_val_loss = logs.get(\"val_loss\")\n",
        "    print(\"\\n current_val_loss: {}\".format(current_val_loss))\n",
        "    temp_model_path = self.model_path.replace(\".h5\", \"_temp.h5\")\n",
        "    self.model.save(temp_model_path)\n",
        "    if current_val_loss < self.best_val_loss:\n",
        "      self.best_val_loss = current_val_loss\n",
        "      self.wait = 0\n",
        "      self.best_weights = self.model.get_weights()\n",
        "      self.model.save(self.model_path)\n",
        "\n",
        "    else:\n",
        "        self.wait += 1\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = epoch\n",
        "            self.is_impatient = True\n",
        "            self.model.stop_training = True\n",
        "            print(\"Restoring model weights from the end of the best epoch.\")\n",
        "            self.model.set_weights(self.best_weights)\n",
        "            #os.remove(temp_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbMh_ABBRv3y"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The CRNN developed for audio segmentation.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "mel_input = keras.Input(shape=(802, 80), name=\"mel_input\")\n",
        "X = mel_input\n",
        "\n",
        "X = tf.keras.layers.Reshape((802, 80, 1))(X)\n",
        "print(X.shape)\n",
        "\n",
        "X = tf.keras.layers.Conv2D(filters=16, kernel_size=7, strides=1, padding='same')(X)\n",
        "X = layers.BatchNormalization(momentum=0.0)(X)\n",
        "X = tf.keras.layers.Activation('relu')(X)\n",
        "X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)\n",
        "X = tf.keras.layers.Dropout(rate = 0.2)(X)\n",
        "\n",
        "X = tf.keras.layers.Conv2D(filters=64, kernel_size=7, strides=1, padding='same')(X)\n",
        "X = layers.BatchNormalization(momentum=0.0)(X)\n",
        "X = tf.keras.layers.Activation('relu')(X)\n",
        "X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)\n",
        "X = layers.Dropout(rate = 0.2)(X)\n",
        "\n",
        "print(X.shape)\n",
        "_, _, sx, sy = X.shape\n",
        "X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)\n",
        "\n",
        "X = layers.Bidirectional(layers.GRU(80, return_sequences = True))(X)\n",
        "X = layers.BatchNormalization(momentum=0.0)(X)\n",
        "\n",
        "X = layers.Bidirectional(layers.GRU(80, return_sequences = True))(X)\n",
        "X = layers.BatchNormalization(momentum=0.0)(X)\n",
        "\n",
        "pred = layers.TimeDistributed(layers.Dense(2, activation='sigmoid'))(X)\n",
        "\n",
        "model = keras.Model(inputs = [mel_input], outputs = [pred])\n",
        "\n",
        "keras.utils.plot_model(model, \"CRNN.png\", show_shapes=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=[keras.losses.BinaryCrossentropy()], metrics=['binary_accuracy'] #, 'categorical_accuracy', tf.keras.metrics.Precision(class_id=0), tf.keras.metrics.Precision(class_id=1), tf.keras.metrics.Recall(class_id=0), tf.keras.metrics.Recall(class_id=1)]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF_cdXjsdI6h"
      },
      "outputs": [],
      "source": [
        "# Ensure the directory for the model path is already created.\n",
        "model_path = \"/content/drive/My Drive/model-1.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARrvIdEM_TCx"
      },
      "outputs": [],
      "source": [
        "history = model.fit(training_generator, validation_data=validation_generator, epochs=30, \n",
        "                    callbacks=[MyCustomCallback(model_path, patience=15)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Lr0Fy5bTl0"
      },
      "outputs": [],
      "source": [
        "model.evaluate(validation_generator)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNdyE7ulVfmySq96HJLJKQM",
      "include_colab_link": true,
      "name": "train-CRNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('py37')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c9c855a44121496af4e6e4c999f16816b9ec462c87e9ab49f53ff2fe6ca5065"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
